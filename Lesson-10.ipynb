{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "amber-safety",
   "metadata": {},
   "source": [
    "## Natural Language Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "biblical-cornwall",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nlp pretrained model -> actually a language model\n",
    "\n",
    "# Language model: Special kind of model that has been\n",
    "# trained to guess what the next word in a text.\n",
    "# we don't need to give labels to model, it has a process\n",
    "# to automatically get labels from the data.\n",
    "\n",
    "#Self supervised learning: Training a model using labels \n",
    "#that are embedded in the independent variable, rather \n",
    "#than requiring external labels. For instance, \n",
    "#training a model to predict the next word in a text.\n",
    "\n",
    "# Universal Language Model Fine-tuning(ULMFit): An extra\n",
    "# stage of fine-tuning of the language model, prior to \n",
    "# transfer learning to a classification task.\n",
    "# so, fine-tune the pretrianed language model, which was\n",
    "# trained only on wikipedia articles; this will result\n",
    "# the model is good at\n",
    "# predicting the next word of the movie review.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "burning-revelation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text preprocessing\n",
    "# Step 1 : first we need to concatenate all of the docx to a long string;\n",
    "# and split into words(tokens)\n",
    "# Step 2: Our independent variable will be the sequence of words\n",
    "# starting with the first work in ut very long list and ending with the\n",
    "# second to last.\n",
    "# Step 3: our dependent variable will be the sequence of words starting \n",
    "# with the second word and ending with the last word\n",
    "\n",
    "# * we use the corresponding row in the embedding matrix, for those\n",
    "# words that are in the vocabulary list of our pretrained model.\n",
    "\n",
    "# * for new words we will just initialize the corresponding row with a \n",
    "# random vector.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "chief-talent",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenization (Convert the text into a list of words)\n",
    "# Token  One element of a list created by the tokenization process. It could be a word, part of a word (a subword), or a single character.\n",
    "# Word-based: Split a sentence on spaces\n",
    "# Subword based: Split words into smaller parts, based on the most commonly\n",
    "# occuring substrings. For instance, \"Occasion\" might be tokenized as \"o c ca sion.\"\n",
    "\n",
    "# Character-based: Split a sentence into its individual characters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "supreme-scholarship",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.text.all import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "studied-permission",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "path = untar_data(URLs.IMDB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "green-triangle",
   "metadata": {},
   "outputs": [],
   "source": [
    "files = get_text_files(path, folders = ['train', 'test', 'unsup'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "electric-spirituality",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Feeling Minnesota, directed by Steven Ba'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "txt = files[0].open().read()\n",
    "txt[:40]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "coupled-width",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(#138) ['Feeling','Minnesota',',','directed','by','Steven','Baigelmann',',','and','starring'...]\n"
     ]
    }
   ],
   "source": [
    "# fastai WordTokenizer uses Spacy util now.\n",
    "spacy = WordTokenizer()\n",
    "toks = first(spacy([txt])) # first index 0 element\n",
    "print(coll_repr(toks, 10)) # coll_repr(collection, n) function to display the results. n items of collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "accompanied-scheduling",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(#11) ['hi','lalkrishna','!','how','are','you',',','what','about','your'...]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first(spacy(['hi lalkrishna! how are you, what about your goals']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "assisted-pharmacy",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "hired-tribe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fastai additional functionality in Tokenizer class\n",
    "tkn = Tokenizer(spacy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "collect-operation",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"(#162) ['xxbos','xxmaj','feeling','xxmaj','minnesota',',','directed','by','xxmaj','steven'...]\""
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coll_repr(tkn(txt),10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "similar-jackson",
   "metadata": {},
   "outputs": [],
   "source": [
    "# xxmaj(beginning of stream) -> next work will start will capital letter\n",
    "# or model need to forget what was said previously and focus on upcoming words.\n",
    "\n",
    "# xxbos -> Start of the document\n",
    "# xxunk -> word is unknown\n",
    "# the reason why we do it, is that the cap version and lower case version \n",
    "# gone be two words in embedding matrixs.\n",
    "# sometimes cap might matter.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "phantom-strengthening",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<function fastai.text.core.fix_html(x)>,\n",
       " <function fastai.text.core.replace_rep(t)>,\n",
       " <function fastai.text.core.replace_wrep(t)>,\n",
       " <function fastai.text.core.spec_add_spaces(t)>,\n",
       " <function fastai.text.core.rm_useless_spaces(t)>,\n",
       " <function fastai.text.core.replace_all_caps(t)>,\n",
       " <function fastai.text.core.replace_maj(t)>,\n",
       " <function fastai.text.core.lowercase(t, add_bos=True, add_eos=False)>]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "defaults.text_proc_rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "governmental-action",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fix_html: replaces special Html characters with a readable version\n",
    "# replace_rep: replaces any character repeated 3 times or more with a special\n",
    "# token for repetition (xxrep), number of times it's repeated, then the character.\n",
    "\n",
    "# spec_add_spaces: adds spaces around / and #\n",
    "# rm_useless_spaces: removes all repetitions of the space character\n",
    "# replace_all_caps: lowercases a word(caps) -> add token (xxup) in front of it\n",
    "# replace_maj: lowercases a capitalized word -> add token(xxmaj)\n",
    "# lowercase: lowecases all text -> add beginning xxbox or at the end xxeos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "funded-furniture",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"(#11) ['xxbos','Â©','xxmaj','fast.ai','xxrep','3','w','.fast.ai','/','xxup','index']\""
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coll_repr(tkn('&copy;   Fast.ai www.fast.ai/INDEX'), 31)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "scientific-philip",
   "metadata": {},
   "outputs": [],
   "source": [
    "coll_repr(tkn('copy;   Fast.ai www.fast.ai/INDEX'), 31)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
