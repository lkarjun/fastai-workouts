{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "strange-asbestos",
   "metadata": {},
   "source": [
    "## Natural Language Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "mediterranean-framing",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nlp pretrained model -> actually a language model\n",
    "\n",
    "# Language model: Special kind of model that has been\n",
    "# trained to guess what the next word in a text.\n",
    "# we don't need to give labels to model, it has a process\n",
    "# to automatically get labels from the data.\n",
    "\n",
    "#Self supervised learning: Training a model using labels \n",
    "#that are embedded in the independent variable, rather \n",
    "#than requiring external labels. For instance, \n",
    "#training a model to predict the next word in a text.\n",
    "\n",
    "# Universal Language Model Fine-tuning(ULMFit): An extra\n",
    "# stage of fine-tuning of the language model, prior to \n",
    "# transfer learning to a classification task.\n",
    "# so, fine-tune the pretrianed language model, which was\n",
    "# trained only on wikipedia articles; this will result\n",
    "# the model is good at\n",
    "# predicting the next word of the movie review.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "minus-observation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text preprocessing\n",
    "# Step 1 : first we need to concatenate all of the docx to a long string;\n",
    "# and split into words(tokens)\n",
    "# Step 2: Our independent variable will be the sequence of words\n",
    "# starting with the first work in ut very long list and ending with the\n",
    "# second to last.\n",
    "# Step 3: our dependent variable will be the sequence of words starting \n",
    "# with the second word and ending with the last word\n",
    "\n",
    "# * we use the corresponding row in the embedding matrix, for those\n",
    "# words that are in the vocabulary list of our pretrained model.\n",
    "\n",
    "# * for new words we will just initialize the corresponding row with a \n",
    "# random vector.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "arbitrary-contributor",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenization (Convert the text into a list of words)\n",
    "# Token  One element of a list created by the tokenization process. It could be a word, part of a word (a subword), or a single character.\n",
    "# Word-based: Split a sentence on spaces\n",
    "# Subword based: Split words into smaller parts, based on the most commonly\n",
    "# occuring substrings. For instance, \"Occasion\" might be tokenized as \"o c ca sion.\"\n",
    "\n",
    "# Character-based: Split a sentence into its individual characters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "secondary-european",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.text.all import *\n",
    "from IPython.display import display,HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "essential-inflation",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = untar_data(URLs.IMDB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "blocked-carroll",
   "metadata": {},
   "outputs": [],
   "source": [
    "files = get_text_files(path, folders = ['train', 'test', 'unsup'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "understanding-scheduling",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Feeling Minnesota, directed by Steven Ba'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "txt = files[0].open().read()\n",
    "txt[:40]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "interim-matrix",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(#138) ['Feeling','Minnesota',',','directed','by','Steven','Baigelmann',',','and','starring'...]\n"
     ]
    }
   ],
   "source": [
    "# fastai WordTokenizer uses Spacy until now.\n",
    "spacy = WordTokenizer()\n",
    "toks = first(spacy([txt])) # first index 0 element\n",
    "print(coll_repr(toks, 10)) # coll_repr(collection, n) function to display the results. n items of collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "regulated-protection",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(#11) ['hi','lalkrishna','!','how','are','you',',','what','about','your'...]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first(spacy(['hi lalkrishna! how are you, what about your goals']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "julian-picking",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fastai additional functionality in Tokenizer class\n",
    "tkn = Tokenizer(spacy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "contrary-blame",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"(#162) ['xxbos','xxmaj','feeling','xxmaj','minnesota',',','directed','by','xxmaj','steven'...]\""
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coll_repr(tkn(txt),10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "architectural-reggae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# xxmaj(beginning of stream) -> next work will start will capital letter\n",
    "# or model need to forget what was said previously and focus on upcoming words.\n",
    "\n",
    "# xxbos -> Start of the document\n",
    "# xxunk -> word is unknown\n",
    "# the reason why we do it, is that the cap version and lower case version \n",
    "# gone be two words in embedding matrixs.\n",
    "# sometimes cap might matter.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "golden-aquatic",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<function fastai.text.core.fix_html(x)>,\n",
       " <function fastai.text.core.replace_rep(t)>,\n",
       " <function fastai.text.core.replace_wrep(t)>,\n",
       " <function fastai.text.core.spec_add_spaces(t)>,\n",
       " <function fastai.text.core.rm_useless_spaces(t)>,\n",
       " <function fastai.text.core.replace_all_caps(t)>,\n",
       " <function fastai.text.core.replace_maj(t)>,\n",
       " <function fastai.text.core.lowercase(t, add_bos=True, add_eos=False)>]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "defaults.text_proc_rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "pleasant-norfolk",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fix_html: replaces special Html characters with a readable version\n",
    "# replace_rep: replaces any character repeated 3 times or more with a special\n",
    "# token for repetition (xxrep), number of times it's repeated, then the character.\n",
    "\n",
    "# spec_add_spaces: adds spaces around / and #\n",
    "# rm_useless_spaces: removes all repetitions of the space character\n",
    "# replace_all_caps: lowercases a word(caps) -> add token (xxup) in front of it\n",
    "# replace_maj: lowercases a capitalized word -> add token(xxmaj)\n",
    "# lowercase: lowecases all text -> add beginning xxbox or at the end xxeos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "expired-composer",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"(#11) ['xxbos','Â©','xxmaj','fast.ai','xxrep','3','w','.fast.ai','/','xxup','index']\""
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coll_repr(tkn('&copy;   Fast.ai www.fast.ai/INDEX'), 31)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "hundred-hampshire",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"(#7) ['xxbos','xxmaj','t4wads','dake','2la','xxmaj','ad']\""
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coll_repr(tkn('T4wads dake 2la Ad'), 31)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "awful-gallery",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "multiple-belgium",
   "metadata": {},
   "source": [
    "## Subwork tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "rising-farming",
   "metadata": {},
   "outputs": [],
   "source": [
    "# some languages don't uses space. to handle these cases\n",
    "# we use subword tokenization -> language like chines, japanese\n",
    "\n",
    "# Step 1: Analyze a corpus of documents to find the most commonly \n",
    "#occurring groups of letters. These become the vocab.\n",
    "\n",
    "# Step 2: Tokenize the corpus using this vocab of subword units."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "optional-reverse",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Feeling Minnesota, directed by Steven Baigelmann, and starring Keanu Reeves'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "txts = L(o.open().read() for o in files[:1000])\n",
    "first(txts)[:75]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "subtle-tanzania",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "valuable-death",
   "metadata": {},
   "outputs": [],
   "source": [
    "def subword(sz):\n",
    "    sp = SubwordTokenizer(vocab_sz=sz) \n",
    "    # this will train/ find the commonly occuring groups of letter\n",
    "    sp.setup(txts)\n",
    "    return ' '.join(first(sp([txt]))[:40])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "referenced-grounds",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\"âF e el ing âMinne so ta , âdirected âby âSteven âBaigelman n , âand â starring âK eanu âReeve s , âCamer on âDiaz âand âVince nt âD ' On of ri o : âThe âstrain ed ârelationship âbetween\""
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subword(6000)\n",
    "# _ is a special character represent space character in the original text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "european-failure",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'â F e e l ing â M in n es o t a , âd i re c t ed âb y âS t e ve n â B a i g e l m an n , âand'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subword(200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "promotional-quilt",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'âF e el ing âM in ne s o t a , â direct ed âby âS te ve n âB a ig el man n , âand âst ar r ing âK e an u âR e e ve'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subword(500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "addressed-forge",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a large vocab means fewer tokens per sentence.\n",
    "# downside -> large embedding matrices. also require more data to learn."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sustainable-planning",
   "metadata": {},
   "source": [
    "## Numericalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "monthly-cargo",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Numericalization with fastai\n",
    "# is the process of mapping tokens to integers.\n",
    "# Step1: Make a list of all possible levels of that categorical variable (the vocab).\n",
    "# Step2: Replace each level with its index in the vocab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "eligible-illness",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(#162) ['xxbos','xxmaj','feeling','xxmaj','minnesota',',','directed','by','xxmaj','steven','xxmaj','baigelmann',',','and','starring','xxmaj','keanu','xxmaj','reeves',',','xxmaj','cameron','xxmaj','diaz','and','xxmaj','vincent',\"d'onofrio\",':','xxmaj','the'...]\n"
     ]
    }
   ],
   "source": [
    "toks = tkn(txt)\n",
    "print(coll_repr(tkn(txt), 31))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "hourly-submission",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(#162) ['xxbos','xxmaj','feeling','xxmaj','minnesota',',','directed','by','xxmaj','steven'...]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "toks400 = txts[:400].map(tkn)\n",
    "first(toks400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "quarterly-freedom",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"(#3312) ['xxunk','xxpad','xxbos','xxeos','xxfld','xxrep','xxwrep','xxup','xxmaj','the','.',',','a','and','of','to','is','i','in','it'...]\""
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num = Numericalize() #min_freq = 3 means any word apper less than 3 times is replaced with xxunk\n",
    "num.setup(toks400)\n",
    "coll_repr(num.vocab, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "attended-boring",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorText([   2,    8,  919,    8,    0,   11,  587,   49,    8, 1647,    8,    0,\n",
       "          11,   13,  985,    8,    0,    8,    0,   11])"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nums = num(toks)[:20]\n",
    "nums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "developed-niger",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'xxbos'"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num.vocab[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "wicked-allah",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'xxbos xxmaj feeling xxmaj xxunk , directed by xxmaj steven xxmaj xxunk , and starring xxmaj xxunk xxmaj xxunk ,'"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "' '.join(num.vocab[o] for o in nums)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "saved-authority",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "settled-galaxy",
   "metadata": {},
   "source": [
    "## Put txts into batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "informative-relations",
   "metadata": {},
   "outputs": [],
   "source": [
    "stream = txt\n",
    "tokens = tkn(stream)\n",
    "bs, seq_len = 6, 15\n",
    "d_tokens = np.array([tokens[i*seq_len:(i+1)*seq_len] for i in range(bs)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "pacific-acrobat",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['xxbos', 'xxmaj', 'feeling', 'xxmaj', 'minnesota', ',', 'directed',\n",
       "       'by', 'xxmaj', 'steven', 'xxmaj', 'baigelmann', ',', 'and',\n",
       "       'starring'], dtype='<U12')"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d_tokens[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "fifteen-development",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(#15) ['xxbos','xxmaj','feeling','xxmaj','minnesota',',','directed','by','xxmaj','steven'...]"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens[0*15:(0+1)*15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "behind-candy",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>xxbos</td>\n",
       "      <td>xxmaj</td>\n",
       "      <td>feeling</td>\n",
       "      <td>xxmaj</td>\n",
       "      <td>minnesota</td>\n",
       "      <td>,</td>\n",
       "      <td>directed</td>\n",
       "      <td>by</td>\n",
       "      <td>xxmaj</td>\n",
       "      <td>steven</td>\n",
       "      <td>xxmaj</td>\n",
       "      <td>baigelmann</td>\n",
       "      <td>,</td>\n",
       "      <td>and</td>\n",
       "      <td>starring</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>xxmaj</td>\n",
       "      <td>keanu</td>\n",
       "      <td>xxmaj</td>\n",
       "      <td>reeves</td>\n",
       "      <td>,</td>\n",
       "      <td>xxmaj</td>\n",
       "      <td>cameron</td>\n",
       "      <td>xxmaj</td>\n",
       "      <td>diaz</td>\n",
       "      <td>and</td>\n",
       "      <td>xxmaj</td>\n",
       "      <td>vincent</td>\n",
       "      <td>d'onofrio</td>\n",
       "      <td>:</td>\n",
       "      <td>xxmaj</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>the</td>\n",
       "      <td>strained</td>\n",
       "      <td>relationship</td>\n",
       "      <td>between</td>\n",
       "      <td>two</td>\n",
       "      <td>brothers</td>\n",
       "      <td>,</td>\n",
       "      <td>xxmaj</td>\n",
       "      <td>sam</td>\n",
       "      <td>(</td>\n",
       "      <td>d'onofrio</td>\n",
       "      <td>)</td>\n",
       "      <td>and</td>\n",
       "      <td>xxmaj</td>\n",
       "      <td>jjaks</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>(</td>\n",
       "      <td>reeves</td>\n",
       "      <td>)</td>\n",
       "      <td>,</td>\n",
       "      <td>is</td>\n",
       "      <td>pushed</td>\n",
       "      <td>to</td>\n",
       "      <td>breaking</td>\n",
       "      <td>point</td>\n",
       "      <td>when</td>\n",
       "      <td>xxmaj</td>\n",
       "      <td>jjaks</td>\n",
       "      <td>arrives</td>\n",
       "      <td>at</td>\n",
       "      <td>xxmaj</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>sam</td>\n",
       "      <td>'s</td>\n",
       "      <td>wedding</td>\n",
       "      <td>and</td>\n",
       "      <td>makes</td>\n",
       "      <td>off</td>\n",
       "      <td>with</td>\n",
       "      <td>the</td>\n",
       "      <td>bride</td>\n",
       "      <td>,</td>\n",
       "      <td>xxmaj</td>\n",
       "      <td>freddie</td>\n",
       "      <td>(</td>\n",
       "      <td>diaz</td>\n",
       "      <td>)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>,</td>\n",
       "      <td>a</td>\n",
       "      <td>former</td>\n",
       "      <td>stripper</td>\n",
       "      <td>,</td>\n",
       "      <td>marrying</td>\n",
       "      <td>xxmaj</td>\n",
       "      <td>sam</td>\n",
       "      <td>to</td>\n",
       "      <td>repay</td>\n",
       "      <td>a</td>\n",
       "      <td>gambling</td>\n",
       "      <td>debt</td>\n",
       "      <td>owed</td>\n",
       "      <td>to</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df = pd.DataFrame(d_tokens)\n",
    "display(HTML(df.to_html(index=False, header=None)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "approved-arrow",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can't directly feed the df bcoz a single batch containing all the texts would fit in our GPU memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "normal-yahoo",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>xxbos</td>\n",
       "      <td>xxmaj</td>\n",
       "      <td>feeling</td>\n",
       "      <td>xxmaj</td>\n",
       "      <td>minnesota</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>xxmaj</td>\n",
       "      <td>keanu</td>\n",
       "      <td>xxmaj</td>\n",
       "      <td>reeves</td>\n",
       "      <td>,</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>the</td>\n",
       "      <td>strained</td>\n",
       "      <td>relationship</td>\n",
       "      <td>between</td>\n",
       "      <td>two</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>(</td>\n",
       "      <td>reeves</td>\n",
       "      <td>)</td>\n",
       "      <td>,</td>\n",
       "      <td>is</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>sam</td>\n",
       "      <td>'s</td>\n",
       "      <td>wedding</td>\n",
       "      <td>and</td>\n",
       "      <td>makes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>,</td>\n",
       "      <td>a</td>\n",
       "      <td>former</td>\n",
       "      <td>stripper</td>\n",
       "      <td>,</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#First Minibatch\n",
    "bs,seq_len = 6,5\n",
    "d_tokens = np.array([tokens[i*15:i*15+seq_len] for i in range(bs)])\n",
    "df = pd.DataFrame(d_tokens)\n",
    "display(HTML(df.to_html(index=False,header=None)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "sufficient-acquisition",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>,</td>\n",
       "      <td>directed</td>\n",
       "      <td>by</td>\n",
       "      <td>xxmaj</td>\n",
       "      <td>steven</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>xxmaj</td>\n",
       "      <td>cameron</td>\n",
       "      <td>xxmaj</td>\n",
       "      <td>diaz</td>\n",
       "      <td>and</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>brothers</td>\n",
       "      <td>,</td>\n",
       "      <td>xxmaj</td>\n",
       "      <td>sam</td>\n",
       "      <td>(</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>pushed</td>\n",
       "      <td>to</td>\n",
       "      <td>breaking</td>\n",
       "      <td>point</td>\n",
       "      <td>when</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>off</td>\n",
       "      <td>with</td>\n",
       "      <td>the</td>\n",
       "      <td>bride</td>\n",
       "      <td>,</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>marrying</td>\n",
       "      <td>xxmaj</td>\n",
       "      <td>sam</td>\n",
       "      <td>to</td>\n",
       "      <td>repay</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Then this one\n",
    "bs,seq_len = 6,5\n",
    "d_tokens = np.array([tokens[i*15+seq_len:i*15+2*seq_len] for i in range(bs)])\n",
    "df = pd.DataFrame(d_tokens)\n",
    "display(HTML(df.to_html(index=False,header=None)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "classical-weekend",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "billion-finish",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The above functionality is achieved through fastai LMDataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "level-macedonia",
   "metadata": {},
   "outputs": [],
   "source": [
    "num400 = [num(i) for i in toks400]\n",
    "# or toks400.map(num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "second-friday",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorText([   2,    8,  919,    8,    0,   11,  587,   49,    8, 1647,    8,    0,\n",
       "          11,   13,  985,    8,    0,    8,    0,   11])"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num400[0][:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "floral-romantic",
   "metadata": {},
   "outputs": [],
   "source": [
    "dl = LMDataLoader(num400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "small-slovak",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([64, 72]), torch.Size([64, 72]))"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x, y = first(dl)\n",
    "x.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "adolescent-plaza",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is we want x and y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "genetic-validity",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'xxbos xxmaj feeling xxmaj xxunk , directed by xxmaj steven xxmaj xxunk , and starring xxmaj xxunk xxmaj xxunk ,'"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "' '.join(num.vocab[o] for o in x[0][:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "frozen-jenny",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'xxmaj feeling xxmaj xxunk , directed by xxmaj steven xxmaj xxunk , and starring xxmaj xxunk xxmaj xxunk , xxmaj'"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "' '.join(num.vocab[o] for o in y[0][:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "breeding-partition",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "affected-professional",
   "metadata": {},
   "source": [
    "## Language model using DataBlock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "natural-verification",
   "metadata": {},
   "outputs": [],
   "source": [
    "partial?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adjustable-airplane",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
